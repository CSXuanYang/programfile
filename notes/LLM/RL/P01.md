强化学习中的随机性
1. 动作：动作是根据policy函数随机抽样得到的。给定当前状态S，agent的动作A是按照policy函数π输出的概率来随机抽样。
2. 状态转移：环境用状态转移函数P算出概率，然后用概率来随机抽样得到下一个状态S<sup>'</sup>。

trajectory：（state，action，reward）

return回报：cumulative future reward未来的累积奖励
reward奖励：只依赖于当前时刻的S和A

动作价值函数Q(s<sub>t</sub>,a<sub>t</sub>),s<sub>t</sub>和a<sub>t</sub>
如果当前状态是S<sub>t</sub>，Q<sup>*</sup>函数告诉我们动作a<sub>t</sub>好不好
给动作a打分

状态价值函数：V<sub>π</sub>(s<sub>t</sub>)：是Q<sub>π</sub>的期望：在某一个策略下，所有动作带来的return的期望

Q<sup>*</sup>可以给状态s下的所有动作打分，agent根据分数来选择。

把Q<sup>*</sup>当做一个先知，他能告诉你每个动作带来的平均回报，
DQN：用一个神经网络去近似Q<sup>*</sup>函数
训练：TD（temperal different）算法

Actor-critic：
策略网络做出动作，价值网络评价动作
更新策略网络的参数θ，是为了让V函数的值增加，V函数是对状态s和策略π的评价，固定s，V越大说明策略π越好，所以我们需要更新θ，使得V的值增加。学习策略网络π时，监督是由价值网络q提供的。
q函数靠环境改的奖励改进，裁判打分越来越精准。

更新价值网络q使用TD算法，