问题：
1. 涌现能力的参数量下限
2. 实现复杂推理能力的最小训练集是多大
3. 特定任务训练
4. 逐字生成结果的必要性：每次只吐一个字是否效率太低
5. Transformer是否是最优解
6. 大模型是具有推理能力还是检索+总结能力
7. 如何评价大模型的智力水平：推理能力
8. 优化训练/执行速度
9. 如何从脑科学借力
10. 涌现能力的可解释性
[参考链接](https://www.zhihu.com/question/595298808/answer/2981476070?utm_campaign=&utm_medium=social&utm_oi=1169983959018020864&utm_psn=1685970397265358848&utm_source=zhihu)

---
## [RLHF解读](https://huggingface.co/blog/zh/rlhf)

RLHF 的思想：使用强化学习的方式直接优化带有人类反馈的语言模型。RLHF 使得在一般文本数据语料库上训练的语言模型能和复杂的人类价值观对齐。
1. 预训练一个语言模型 (LM) ；
2. 聚合问答数据并训练一个奖励模型 (Reward Model，RM) ；
   随机抽样一批prompt（大部分和第一阶段相同），对于每个prompt，由第一阶段微调好的模型产生k个答案，<prompt,answer1>,<prompt,answer2>….<prompt,answerK>，之后人工对k个结果排序。采取的训练模式其实就是平常经常用到的pair-wise learning to rank。对于K个排序结果，两两组合，形成 
 个训练数据对，ChatGPT采取pair-wise loss来训练Reward Model。
   用这个排序结果训练奖励模型
3. 用强化学习 (RL) 方式微调 LM。
   首先，该 策略 (policy) 是一个接受提示并返回一系列文本 (或文本的概率分布) 的 LM。这个策略的 行动空间 (action space) 是 LM 的词表对应的所有词元 (一般在 50k 数量级) ，观察空间 (observation space) 是可能的输入词元序列，也比较大 (词汇量 ^ 输入标记的数量) 。奖励函数 是偏好模型和策略转变约束 (Policy shift constraint) 的结合。

参考链接：
[ChatGPT会取代搜索引擎吗](https://zhuanlan.zhihu.com/p/589533490)
[详解大模型RLHF过程（配代码解读](https://zhuanlan.zhihu.com/p/624589622)
[ChatGPT 背后的“功臣”——RLHF 技术详解](https://huggingface.co/blog/zh/rlhf)
[从零实现ChatGPT——RLHF技术笔记](https://zhuanlan.zhihu.com/p/591474085)
[抱抱脸：ChatGPT背后的算法——RLHF | 附12篇RLHF必刷论文](https://zhuanlan.zhihu.com/p/592671478)

### PPO

强化学习两大流派：
- 从reward来学习
- 基于policy：将最终的奖励最大化，相当于优化目标函数
  - PPO就是基于policy的
  - 类比监督学习：需要从延迟性的，间接奖励学习 
  
  **关于PPO，需要学习一下李宏毅的强化学习课，今天估计看不了了**